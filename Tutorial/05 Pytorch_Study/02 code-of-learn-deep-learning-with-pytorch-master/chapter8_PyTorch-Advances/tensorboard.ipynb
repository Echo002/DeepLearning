{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard 可视化\n",
    "[github](https://github.com/lanpa/tensorboard-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T14:23:41.438578Z",
     "start_time": "2019-03-09T14:23:40.881923Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import CIFAR10\n",
    "from utils import resnet\n",
    "from torchvision import transforms as tfs\n",
    "from datetime import datetime\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T14:23:42.560435Z",
     "start_time": "2019-03-09T14:23:42.410836Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7eb079092d67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mvalid_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\torchvision\\datasets\\cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             raise RuntimeError('Dataset not found or corrupted.' +\n\u001b[1;32m---> 62\u001b[1;33m                                ' You can use download=True to download it')\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# now load the picked numpy arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "# 使用数据增强\n",
    "def train_tf(x):\n",
    "    im_aug = tfs.Compose([\n",
    "        tfs.Resize(120),\n",
    "        tfs.RandomHorizontalFlip(),\n",
    "        tfs.RandomCrop(96),\n",
    "        tfs.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5),\n",
    "        tfs.ToTensor(),\n",
    "        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    x = im_aug(x)\n",
    "    return x\n",
    "\n",
    "def test_tf(x):\n",
    "    im_aug = tfs.Compose([\n",
    "        tfs.Resize(96),\n",
    "        tfs.ToTensor(),\n",
    "        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    x = im_aug(x)\n",
    "    return x\n",
    "\n",
    "train_set = CIFAR10('./data', train=True, transform=train_tf)\n",
    "train_data = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=4)\n",
    "valid_set = CIFAR10('./data', train=False, transform=test_tf)\n",
    "valid_data = torch.utils.data.DataLoader(valid_set, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "net = resnet(3, 10)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:39:16.284525Z",
     "start_time": "2019-03-09T13:39:15.648198Z"
    }
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "def get_acc(output, label):\n",
    "    total = output.shape[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    num_correct = (pred_label == label).sum().data[0]\n",
    "    return num_correct / total\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "prev_time = datetime.now()\n",
    "for epoch in range(30):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    net = net.train()\n",
    "    for im, label in train_data:\n",
    "        if torch.cuda.is_available():\n",
    "            im = Variable(im.cuda())  # (bs, 3, h, w)\n",
    "            label = Variable(label.cuda())  # (bs, h, w)\n",
    "        else:\n",
    "            im = Variable(im)\n",
    "            label = Variable(label)\n",
    "        # forward\n",
    "        output = net(im)\n",
    "        loss = criterion(output, label)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        train_acc += get_acc(output, label)\n",
    "    cur_time = datetime.now()\n",
    "    h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    net = net.eval()\n",
    "    for im, label in valid_data:\n",
    "        if torch.cuda.is_available():\n",
    "            im = Variable(im.cuda(), volatile=True)\n",
    "            label = Variable(label.cuda(), volatile=True)\n",
    "        else:\n",
    "            im = Variable(im, volatile=True)\n",
    "            label = Variable(label, volatile=True)\n",
    "        output = net(im)\n",
    "        loss = criterion(output, label)\n",
    "        valid_loss += loss.data[0]\n",
    "        valid_acc += get_acc(output, label)\n",
    "    epoch_str = (\n",
    "                \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n",
    "                % (epoch, train_loss / len(train_data),\n",
    "                   train_acc / len(train_data), valid_loss / len(valid_data),\n",
    "                   valid_acc / len(valid_data)))\n",
    "    prev_time = cur_time\n",
    "    # ====================== 使用 tensorboard ==================\n",
    "    writer.add_scalars('Loss', {'train': train_loss / len(train_data),\n",
    "                                'valid': valid_loss / len(valid_data)}, epoch)\n",
    "    writer.add_scalars('Acc', {'train': train_acc / len(train_data),\n",
    "                               'valid': valid_acc / len(valid_data)}, epoch)\n",
    "    # =========================================================\n",
    "    print(epoch_str + time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T14:24:49.452636Z",
     "start_time": "2019-03-09T14:24:49.293063Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(     #input_size=(1*28*28)\n",
    "            nn.Conv2d(1, 6, 5, 1, 2),\n",
    "            nn.ReLU(),      #(6*28*28)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  #output_size=(6*14*14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),      #(16*10*10)\n",
    "            nn.MaxPool2d(2, 2)  #output_size=(16*5*5)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # 定义前向传播过程，输入为x\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "dummy_input = torch.rand(13, 1, 28, 28) #假设输入13张1*28*28的图片\n",
    "model = LeNet()\n",
    "with SummaryWriter(comment='LeNet') as w:\n",
    "    w.add_graph(model, (dummy_input, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ws1.sinaimg.cn/large/006tNc79ly1fms31s3i4yj31gc0qimy6.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
