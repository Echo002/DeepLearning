{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG\n",
    "计算机视觉是一直深度学习的主战场，从这里我们将接触到近几年非常流行的卷积网络结构，网络结构由浅变深，参数越来越多，网络有着更多的跨层链接，首先我们先介绍一个数据集 cifar10，我们将以此数据集为例介绍各种卷积网络的结构。\n",
    "\n",
    "## CIFAR 10\n",
    "cifar 10 这个数据集一共有 50000 张训练集，10000 张测试集，两个数据集里面的图片都是 png 彩色图片，图片大小是 32 x 32 x 3，一共是 10 分类问题，分别为飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。这个数据集是对网络性能测试一个非常重要的指标，可以说如果一个网络在这个数据集上超过另外一个网络，那么这个网络性能上一定要比另外一个网络好，目前这个数据集最好的结果是 95% 左右的测试集准确率。\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmpjxxq7wcj30db0ae7ag.jpg)\n",
    "\n",
    "你能用肉眼对这些图片进行分类吗？\n",
    "\n",
    "cifar 10 已经被 pytorch 内置了，使用非常方便，只需要调用 `torchvision.datasets.CIFAR10` 就可以了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet\n",
    "vggNet 是第一个真正意义上的深层网络结构，其是 ImageNet2014年的冠军，得益于 python 的函数和循环，我们能够非常方便地构建重复结构的深层网络。\n",
    "\n",
    "vgg 的网络结构非常简单，就是不断地堆叠卷积层和池化层，下面是一个简单的图示\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79ly1fmpk5smtidj307n0dx3yv.jpg)\n",
    "\n",
    "vgg 几乎全部使用 3 x 3 的卷积核以及 2 x 2 的池化层，使用小的卷积核进行多层的堆叠和一个大的卷积核的感受野是相同的，同时小的卷积核还能减少参数，同时可以有更深的结构。\n",
    "\n",
    "vgg 的一个关键就是使用很多层 3 x 3 的卷积然后再使用一个最大池化层，这个模块被使用了很多次，下面我们照着这个结构来写一写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:43:02.287527Z",
     "start_time": "2019-03-05T12:42:59.300462Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以定义一个 vgg 的 block，传入三个参数，第一个是模型层数，第二个是输入的通道数，第三个是输出的通道数，第一层卷积接受的输入通道就是图片输入的通道数，然后输出最后的输出通道数，后面的卷积接受的通道数就是最后的输出通道数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:43:02.757223Z",
     "start_time": "2019-03-05T12:43:02.749244Z"
    }
   },
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    net = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(True)] # 定义第一层\n",
    "    \n",
    "    for i in range(num_convs-1): # 定义后面的很多层\n",
    "        net.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        net.append(nn.ReLU(True))\n",
    "        \n",
    "    net.append(nn.MaxPool2d(2, 2)) # 定义池化层\n",
    "    return nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将模型打印出来看看结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:43:03.873239Z",
     "start_time": "2019-03-05T12:43:03.853294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace)\n",
      "  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace)\n",
      "  (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): ReLU(inplace)\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "block_demo = vgg_block(3, 64, 128)\n",
    "print(block_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:43:05.685395Z",
     "start_time": "2019-03-05T12:43:04.333010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 150, 150])\n"
     ]
    }
   ],
   "source": [
    "# 首先定义输入为 (1, 64, 300, 300)\n",
    "input_demo = Variable(torch.zeros(1, 64, 300, 300))\n",
    "output_demo = block_demo(input_demo)\n",
    "print(output_demo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到输出就变为了 (1, 128, 150, 150)，可以看到经过了这一个 vgg block，输入大小被减半，通道数变成了 128\n",
    "\n",
    "下面我们定义一个函数对这个 vgg block 进行堆叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:43:05.695369Z",
     "start_time": "2019-03-05T12:43:05.687391Z"
    }
   },
   "outputs": [],
   "source": [
    "def vgg_stack(num_convs, channels):\n",
    "    net = []\n",
    "    for n, c in zip(num_convs, channels):\n",
    "        in_c = c[0]\n",
    "        out_c = c[1]\n",
    "        net.append(vgg_block(n, in_c, out_c))\n",
    "    return nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为实例，我们定义一个稍微简单一点的 vgg 结构，其中有 8 个卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:43:05.865913Z",
     "start_time": "2019-03-05T12:43:05.699358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg_net = vgg_stack((1, 1, 2, 2, 2), ((3, 64), (64, 128), (128, 256), (256, 512), (512, 512)))\n",
    "print(vgg_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到网络结构中有个 5 个 最大池化，说明图片的大小会减少 5 倍，我们可以验证一下，输入一张 256 x 256 的图片看看结果是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:43:07.506528Z",
     "start_time": "2019-03-05T12:43:07.205334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "test_x = Variable(torch.zeros(1, 3, 256, 256))\n",
    "test_y = vgg_net(test_x)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到图片减小了 $2^5$ 倍，最后再加上几层全连接，就能够得到我们想要的分类输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:43:08.237575Z",
     "start_time": "2019-03-05T12:43:08.230595Z"
    }
   },
   "outputs": [],
   "source": [
    "class vgg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg, self).__init__()\n",
    "        self.feature = vgg_net\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以训练我们的模型看看在 cifar10 上的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:43:11.415120Z",
     "start_time": "2019-03-05T12:43:09.189031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "def data_tf(x):\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到\n",
    "    x = x.transpose((2, 0, 1)) # 将 channel 放到第一维，只是 pytorch 要求的输入方式\n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "     \n",
    "train_set = CIFAR10('./data', train=True, transform=data_tf, download=True)\n",
    "train_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_set = CIFAR10('./data', train=False, transform=data_tf, )\n",
    "test_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "\n",
    "net = vgg()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:13:18.985322Z",
     "start_time": "2019-03-05T12:43:11.418095Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Code\\DeepLearning\\Tutorial\\05 Pytorch_Study\\02 code-of-learn-deep-learning-with-pytorch-master\\chapter4_CNN\\utils.py:52: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  im = Variable(im.cuda(), volatile=True)\n",
      "E:\\Code\\DeepLearning\\Tutorial\\05 Pytorch_Study\\02 code-of-learn-deep-learning-with-pytorch-master\\chapter4_CNN\\utils.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  label = Variable(label.cuda(), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 2.303067, Train Acc: 0.100104, Valid Loss: 2.302996, Valid Acc: 0.098892, Time 00:01:27\n",
      "Epoch 1. Train Loss: 2.303067, Train Acc: 0.097806, Valid Loss: 2.302593, Valid Acc: 0.098892, Time 00:01:30\n",
      "Epoch 2. Train Loss: 2.257561, Train Acc: 0.127617, Valid Loss: 2.128244, Valid Acc: 0.168216, Time 00:01:30\n",
      "Epoch 3. Train Loss: 1.952592, Train Acc: 0.233176, Valid Loss: 1.852710, Valid Acc: 0.281547, Time 00:01:30\n",
      "Epoch 4. Train Loss: 1.724399, Train Acc: 0.331502, Valid Loss: 1.667190, Valid Acc: 0.356309, Time 00:01:30\n",
      "Epoch 5. Train Loss: 1.526227, Train Acc: 0.419617, Valid Loss: 1.595382, Valid Acc: 0.424941, Time 00:01:30\n",
      "Epoch 6. Train Loss: 1.325905, Train Acc: 0.515945, Valid Loss: 1.398281, Valid Acc: 0.497528, Time 00:01:30\n",
      "Epoch 7. Train Loss: 1.120888, Train Acc: 0.595249, Valid Loss: 1.085596, Valid Acc: 0.600079, Time 00:01:30\n",
      "Epoch 8. Train Loss: 0.936816, Train Acc: 0.667359, Valid Loss: 1.261605, Valid Acc: 0.578521, Time 00:01:30\n",
      "Epoch 9. Train Loss: 0.777499, Train Acc: 0.727961, Valid Loss: 1.053013, Valid Acc: 0.647943, Time 00:01:30\n",
      "Epoch 10. Train Loss: 0.659450, Train Acc: 0.770520, Valid Loss: 0.905129, Valid Acc: 0.701246, Time 00:01:30\n",
      "Epoch 11. Train Loss: 0.537296, Train Acc: 0.815098, Valid Loss: 0.741210, Valid Acc: 0.743473, Time 00:01:30\n",
      "Epoch 12. Train Loss: 0.447096, Train Acc: 0.845588, Valid Loss: 1.000681, Valid Acc: 0.698279, Time 00:01:30\n",
      "Epoch 13. Train Loss: 0.352217, Train Acc: 0.876898, Valid Loss: 1.526757, Valid Acc: 0.635680, Time 00:01:30\n",
      "Epoch 14. Train Loss: 0.279233, Train Acc: 0.903992, Valid Loss: 0.815360, Valid Acc: 0.764142, Time 00:01:30\n",
      "Epoch 15. Train Loss: 0.223531, Train Acc: 0.922255, Valid Loss: 1.002706, Valid Acc: 0.747033, Time 00:01:30\n",
      "Epoch 16. Train Loss: 0.174253, Train Acc: 0.941057, Valid Loss: 1.449996, Valid Acc: 0.678699, Time 00:01:30\n",
      "Epoch 17. Train Loss: 0.141253, Train Acc: 0.951706, Valid Loss: 1.179455, Valid Acc: 0.732298, Time 00:01:30\n",
      "Epoch 18. Train Loss: 0.112066, Train Acc: 0.962296, Valid Loss: 1.046964, Valid Acc: 0.763350, Time 00:01:30\n",
      "Epoch 19. Train Loss: 0.097322, Train Acc: 0.966073, Valid Loss: 1.044801, Valid Acc: 0.773240, Time 00:01:30\n"
     ]
    }
   ],
   "source": [
    "train(net, train_data, test_data, 20, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，跑完 20 次，vgg 能在 cifar 10 上取得 76% 左右的测试准确率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
